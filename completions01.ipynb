{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completions API\n",
    "\n",
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なモジュールをインポート\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# 環境変数の取得\n",
    "load_dotenv()\n",
    "# OpenAI APIクライアントを生成\n",
    "client = OpenAI(api_key=os.environ['API_KEY'])\n",
    "\n",
    "# モデル名\n",
    "model_name = \"gpt-3.5-turbo-instruct\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パラメーター：prompt、echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "言語モデルを使う上でのポイントは、下記の点が重要であることです。\n",
      "\n",
      "- **現実世界のオーダーでトレーニングデータのオーダーを調整**する。<br>\n",
      "事前に用意した比較的大きなテキストデータからトレーニング用のデータを切り出す際には、比較的おおきなテキストリストを一度に学習するよう並び替え、随時大きなミニバッチを作成してメモリに流し込みます。ただし、この場合、メモリサイズやトレーニングのコンピューティングパワーが厳しくなる可能性があります。そのような場合ライブラリー内で、テキストデータの一部をシャッフルランダムにしてテキストデータの一部だけを使用するようにしたりすると、コンピューターの利用能力をさらに減らすことが可能です。\n",
      "\n",
      "- **順方向と逆方向のモデル\n"
     ]
    }
   ],
   "source": [
    "# プロンプトの設定\n",
    "prompt = \"言語モデルを使う上でのポイントは\"\n",
    "\n",
    "# APIへリクエスト\n",
    "response = client.completions.create(\n",
    "    model=model_name,\n",
    "    prompt=prompt,\n",
    "    max_tokens=300,\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パラメーター：suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "、\n",
      "\n",
      "1. テキストデータの収集と前処理\n",
      "言語モデルを学習させるためには、多くのテキストデータが必要です。そのため、まずは大量のテキストデータを収集し、前処理を行う必要があります。前処理には、テキストのクレンジングやトークナイズ（単語や文に区切る処理）などが含まれます。\n",
      "\n",
      "2. モデルの選択\n",
      "言語モデルには様々な種類がありますが、どのモデルを選択するかは、使用するデータやタスクによって異なります。また、モデルの精度や適用範囲も異なるため、目的に合ったモデルを選択することが重要です。\n",
      "\n",
      "3. モデルの学習\n",
      "言語モデルを実際に学習する際には、適切なハイ\n"
     ]
    }
   ],
   "source": [
    "# プロンプトの設定\n",
    "prompt = \"言語モデルを使う上でのポイントは\"\n",
    "# 末尾文字の設定\n",
    "suffix = \"を続けることです。\"\n",
    "\n",
    "# APIへリクエスト\n",
    "response = client.completions.create(\n",
    "    model=model_name,\n",
    "    prompt=prompt,\n",
    "    max_tokens=300,\n",
    "    suffix=suffix\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パラメーター：best_of\n",
    "\n",
    "\n",
    "### 指定しない場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "、以下のとおりです。\n",
      "\n",
      "> - **前向き確率**を求めることで生成対象となる文章が生成できます。\n",
      "> $$\n",
      "P(w_1,w_2,w\n",
      "--------------------\n",
      "following：\n",
      "\n",
      "### 1.双方向RNNを使うべき\n",
      "入力一単語だけを考えても単語の一つ一つの前後からそれぞれ\n",
      "CompletionUsage(completion_tokens=100, prompt_tokens=17, total_tokens=117)\n"
     ]
    }
   ],
   "source": [
    "# プロンプトの設定\n",
    "prompt = \"言語モデルを使う上でのポイントは\"\n",
    "\n",
    "# APIへリクエスト\n",
    "response = client.completions.create(\n",
    "    model=model_name,\n",
    "    prompt=prompt,\n",
    "    max_tokens=50,\n",
    "    n=2,\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "for choice in response.choices:\n",
    "    print(\"-\" * 20)\n",
    "    print(choice.text.strip())\n",
    "\n",
    "print(response.usage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指定した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "次の3つです。\n",
      "\n",
      "1. 学習データの用意\n",
      "2. 多様性のある文章の生成\n",
      "3. 多様性の評価\n",
      "\n",
      "1\n",
      "--------------------\n",
      "２つあります。\n",
      "一つ目は、言語モデルの拡張性です。言語モデルは、新しい言語データが与えられた場\n",
      "CompletionUsage(completion_tokens=199, prompt_tokens=17, total_tokens=216)\n"
     ]
    }
   ],
   "source": [
    "# プロンプトの設定\n",
    "prompt = \"言語モデルを使う上でのポイントは\"\n",
    "\n",
    "# APIへリクエスト\n",
    "response = client.completions.create(\n",
    "    model=model_name,\n",
    "    prompt=prompt,\n",
    "    max_tokens=50,\n",
    "    n=2,\n",
    "    best_of=4\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "for choice in response.choices:\n",
    "    print(\"-\" * 20)\n",
    "    print(choice.text.strip())\n",
    "\n",
    "print(response.usage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パラメーター：logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKチョコです。\n",
      "\n",
      "こんにちは！OKチョコです。\n",
      "\n",
      "【Q】著者は間もなく、外国に行く予定です！！\n",
      "\n",
      "それは素晴らしいですね！外国に行くと新しい文化や言語、風景を体験できてとても楽しいと思います。準備はしっかりとして、\n"
     ]
    }
   ],
   "source": [
    "# プロンプトの設定\n",
    "prompt = \"こんにちは！\"\n",
    "\n",
    "# APIへリクエスト\n",
    "response = client.completions.create(\n",
    "    model=model_name,\n",
    "    prompt=prompt,\n",
    "    max_tokens=100,\n",
    "    logprobs=3\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tiktoken.core import Encoding\n",
    "\n",
    "# OpenAI APIの特定のモデルに対応するトークナイザーを取得\n",
    "encoding: Encoding = tiktoken.encoding_for_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ OK ] [ チ ] [ ョ ] [ コ ] [ です ] [ 。 ] [ こんにちは ] [ ！ ] [ OK ] [ チ ] [ ョ ] [ コ ] [ です ] [ 。 ] [ 【 ] [ Q ] [ 】 ] [ bytes:\\xe8 ] [ bytes:\\x91 ] [ bytes:\\x97 ] [ 者 ] [ は ] [ 間 ] [ も ] [ な ] [ く ] [ 、 ] [ 外 ] [ 国 ] [ に ] [ 行 ] [ く ] [ bytes:\\xe4\\xba ] [ bytes:\\x88 ] [ 定 ] [ です ] [ ！！ ] [ それ ] [ は ] [ 素 ] [ bytes:\\xe6\\x99 ] [ bytes:\\xb4 ] [ ら ] [ し ] [ い ] [ です ] [ bytes:\\xe3\\x81 ] [ bytes:\\xad ] [ ！ ] [ 外 ] [ 国 ] [ に ] [ 行 ] [ く ] [ と ] [ 新 ] [ し ] [ い ] [ 文 ] [ 化 ] [ や ] [ 言 ] [ bytes:\\xe8\\xaa ] [ bytes:\\x9e ] [ 、 ] [ bytes:\\xe9\\xa2 ] [ bytes:\\xa8 ] [ 景 ] [ を ] [ 体 ] [ bytes:\\xe9 ] [ bytes:\\xa8 ] [ bytes:\\x93 ] [ で ] [ き ] [ て ] [ と ] [ て ] [ も ] [ bytes:\\xe6\\xa5 ] [ bytes:\\xbd ] [ し ] [ い ] [ と ] [ 思 ] [ います ] [ 。 ] [ bytes:\\xe6\\xba ] [ bytes:\\x96 ] [ bytes:\\xe5 ] [ bytes:\\x82 ] [ bytes:\\x99 ] [ は ] [ bytes:\\xe3\\x81\\x97\\xe3\\x81 ] [ bytes:\\xa3 ] [ か ] [ り ] [ と ] [ して ] [ 、 ] "
     ]
    }
   ],
   "source": [
    "# 回答をトークンごとに確認\n",
    "for token in response.choices[0].logprobs.tokens:\n",
    "    print(\"[\", token.replace(\"\\n\",\"\"), \"]\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-9999.0, -9999.0, -3.2188334, -0.67568845, -1.1758041]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選択されたトークンの確率を表示\n",
    "response.choices[0].logprobs.token_logprobs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'\\n': -1.2493514, '\\n\\n': -1.9412345, '私': -2.8816147},\n",
       " {'です': -0.9998541, 'I': -2.9841642, 'Wave': -3.4134445},\n",
       " {'bytes:\\\\xe3\\\\x83\\\\xbc\\\\xe3\\\\x83': -0.81417227,\n",
       "  'ャ': -1.4873176,\n",
       "  'ェ': -2.1293902},\n",
       " {'コ': -0.67568845, 'イ': -2.2441642, 'ッ': -2.6662033},\n",
       " {'です': -1.1758041, 'レ': -1.2034893, 'ラ': -3.4258327}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# トークン上位logprobs個の確率を表示\n",
    "response.choices[0].logprobs.top_logprobs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
